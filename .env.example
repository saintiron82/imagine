# ImageParser Environment Configuration
# Copy this file to .env and adjust settings as needed

# ============================================
# Vision Backend Configuration
# ============================================

# Vision backend: 'transformers' (development) or 'ollama' (deployment)
# - transformers: High accuracy, requires GPU/CPU resources, loads Hugging Face models
# - ollama: Memory-efficient, automatic model management (keep_alive=0)
VISION_BACKEND=transformers

# Vision model (backend-specific):
# For transformers:
#   - Salesforce/blip-image-captioning-large (default, stable)
#   - Salesforce/blip2-opt-2.7b (BLIP-2, higher quality)
#   - Qwen/Qwen2-VL-2B-Instruct (recommended, best quality)
#   - microsoft/Florence-2-large (if flash_attn issue resolved)
# For ollama:
#   - qwen2.5-vl:7b (default)
#   - llava:7b
#   - bakllava:7b
VISION_MODEL=Qwen/Qwen2-VL-2B-Instruct

# Device for transformers backend: 'cuda', 'cpu', or auto-detect (leave blank)
# VISION_DEVICE=cuda

# Ollama server URL (only used when VISION_BACKEND=ollama)
OLLAMA_HOST=http://localhost:11434

# ============================================
# PostgreSQL Configuration
# ============================================

# PostgreSQL connection
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=imageparser
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password

# ============================================
# Output Configuration
# ============================================

# Output directories
OUTPUT_DIR=output
THUMBNAIL_DIR=output/thumbnails
JSON_DIR=output/json

# ============================================
# Performance Tuning
# ============================================

# CLIP model caching (reduce memory usage)
CLIP_CACHE_SIZE=100

# Batch processing
BATCH_SIZE=10

# Translation chunking
TRANSLATION_CHUNK_SIZE=4000
