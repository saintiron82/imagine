# 정확도 평가 계획서 (대조군 대비) - 2026-02-16

## 1. 문서 목적
- 검색 시스템 개선안(B안)이 기존 시스템(A안, 대조군)보다 실제로 더 정확한지 검증한다.
- 개인/게임회사/이미지 자료 회사 환경에서 공통으로 재사용 가능한 평가 체계를 정의한다.
- 오프라인(정답셋 기반) + 온라인(실사용 A/B) 모두에서 의사결정 가능한 기준을 만든다.

## 2. 평가 질문 (Decision Questions)
1. B안이 A안 대비 검색 품질을 유의미하게 개선하는가?
2. 개선이 특정 쿼리 유형에서만 발생하는가, 전반적인가?
3. 정확도 개선이 지연시간/안정성 악화 없이 가능한가?

## 3. 실험 대상/범위
- 대상 시스템: 현재 운영 검색 파이프라인(A) vs 개선 파이프라인(B)
- 동일 조건:
- 동일 데이터셋/인덱스 스냅샷
- 동일 쿼리셋
- 동일 하드웨어 클래스(또는 하드웨어 보정값 적용)
- 평가 모드:
- 오프라인 정적 평가 (필수)
- 온라인 사용자 A/B (선택, 오프라인 통과 후 진행)

## 4. 대조군 정의
- A안(대조군): 현재 프로덕션 설정(가중치/모델/후처리 포함)
- B안(실험군): 개선안 1개만 변경한 버전(한 번에 한 변수 원칙)
- 버전 기록 필수:
- `system_version`
- `model_spec` (vlm, visual, text model + revision/hash)
- `search_config` (가중치, threshold, candidate cut)

## 5. 벤치마크셋 설계 (Offline Gold Set)

### 5.1 크기 및 구성
- 권장 최소: 쿼리 300개
- 권장 목표: 쿼리 500개
- 쿼리 유형 비율(권장):
- 정확 키워드: 35%
- 의미/서술형: 35%
- 시각 유사형(이미지/스타일): 20%
- 혼합/복합 조건: 10%

### 5.2 샘플링 원칙
- 실제 사용 로그 기반 우선(없으면 도메인 전문가 시나리오 생성)
- 고빈도/저빈도 쿼리 균형 포함
- 쉬운 쿼리/어려운 쿼리 균형 포함

### 5.3 정답 라벨링
- 라벨 단위: `(query, doc_id)` relevance 등급
- 등급 체계(권장): `0=무관, 1=관련, 2=매우 관련`
- 라벨러 2명 이상 독립 라벨링
- 불일치 항목은 리뷰어 1명이 최종 adjudication
- 품질 체크:
- Cohen's Kappa 또는 Krippendorff's Alpha 보고
- 목표: `κ >= 0.6` 이상

## 6. 오프라인 평가 지표

### 6.1 주지표 (Primary)
- `nDCG@10` (순위 품질)

### 6.2 보조지표 (Secondary)
- `Recall@20` (정답 회수율)
- `MRR@10` (첫 정답 도달 속도)
- `Precision@5` (초기 화면 품질)

### 6.3 안정성/성능 가드레일
- `p95 query latency`
- `error rate`
- `zero-result rate`

### 6.4 보고 단위
- 전체 평균
- 쿼리 유형별 분리
- 상위/하위 난이도 구간 분리

## 7. 통계 검정 계획
- 단위: query-level paired 비교 (A와 B를 동일 쿼리에서 비교)
- 기본 방법:
- 부트스트랩(10,000회)으로 지표 차이의 95% CI 산출
- 필요 시 대응표본 검정(Wilcoxon signed-rank) 병행
- 판정 규칙:
- `nDCG@10`의 95% CI 하한 > 0 이면 개선으로 판정
- 가드레일 위반 시 배포 보류

## 8. 합격 기준 (Go/No-Go)
- 필수 조건:
- `nDCG@10` 상대 개선 `+8%` 이상
- `Recall@20` 하락 없음
- `p95 latency` 악화 `20%` 이내
- `error rate` 증가 없음
- 권장 조건:
- 유형별 지표에서 1개 이상 하위군 개선 확인
- 하위 20% 난이도 구간에서 성능 악화 없음

## 9. 온라인 A/B 실험 계획 (선택)

### 9.1 시작 조건
- 오프라인 합격 기준 충족 시에만 진행

### 9.2 트래픽 분배
- 1단계: 10% (B), 90% (A)
- 2단계: 25% (B), 75% (A)
- 3단계: 50%/50%

### 9.3 온라인 지표
- `Time to first useful result`
- `query reformulation rate`
- `success@k` (클릭 + 체류 기준)
- `session success rate`
- 가드레일: latency/error/zero-result

### 9.4 중단 기준
- 오류율 급증
- p95 latency 임계 초과
- 핵심 성공지표 유의미 악화

## 10. 데이터/실험 관리 규칙
- 실험마다 불변 아티팩트 저장:
- 쿼리셋 버전
- 정답셋 버전
- 인덱스 스냅샷 ID
- 모델/설정 해시
- 실험 로그 파일 경로
- 평가 스크립트 버전
- 재현성 원칙:
- 동일 입력에서 동일 결과 재현 가능해야 함

## 11. 실행 일정 (초안)
- Day 1~3: 쿼리셋 수집/정제
- Day 4~7: 라벨링 + 합의
- Day 8: 대조군/실험군 러닝
- Day 9: 통계 분석
- Day 10: 결과 리뷰 및 Go/No-Go

## 12. 역할/책임
- PM/오너: 합격 기준 승인, 최종 의사결정
- 검색 엔지니어: A/B 설정 및 실행
- 라벨러(도메인 전문가): 정답셋 구축
- 리뷰어: 라벨 불일치 조정
- 분석 담당: 통계 검정 및 결과 리포트

## 13. 산출물
- `benchmark_queries_v1.jsonl`
- `relevance_labels_v1.jsonl`
- `run_A_*.jsonl`, `run_B_*.jsonl`
- `evaluation_report_v1.md` (지표/통계/판정)

## 14. 리스크 및 대응
- 리스크: 쉬운 쿼리 과대표집 -> 대응: 난이도 층화 샘플링
- 리스크: 라벨 편향 -> 대응: 다중 라벨러 + 합의 프로세스
- 리스크: 테스트셋 과적합 -> 대응: 홀드아웃 셋 분리
- 리스크: 환경 차이 -> 대응: 동일 스냅샷/동일 하드웨어 클래스 보장

## 15. 즉시 착수 체크리스트
- [ ] A안/대조군 버전 고정
- [ ] B안 변경점 1개로 제한
- [ ] 쿼리셋 초안 300개 확보
- [ ] 라벨링 가이드 배포
- [ ] 평가 스크립트 입출력 스키마 확정
- [ ] 합격 기준 사전 서명(변경 금지)

---

이 계획서는 “성능이 좋아 보인다”가 아니라,  
“대조군 대비 유의미하게 개선되었다”를 증명하기 위한 최소 실행 표준이다.

