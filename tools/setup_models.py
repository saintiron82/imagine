#!/usr/bin/env python3
"""
Ollama ëª¨ë¸ ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ - ImageParser v3.1

í‹°ì–´ë³„ AI ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³  ì„¤ì¹˜í•©ë‹ˆë‹¤.

Usage:
    python tools/setup_models.py --tier=standard
    python tools/setup_models.py --tier=pro
    python tools/setup_models.py --tier=ultra
    python tools/setup_models.py --all
"""

import argparse
import subprocess
import sys
import json
from typing import List, Dict


# í‹°ì–´ë³„ Ollama ëª¨ë¸ ëª©ë¡
TIER_MODELS: Dict[str, List[str]] = {
    "standard": [
        # Standard tier: Lightweight models for â‰¤6GB VRAM
        "qwen3-vl:2b",              # VLM: Qwen3-VL-2B-Instruct
        "qwen3-embedding:0.6b",     # MV model: Qwen3-Embedding-0.6B
    ],
    "pro": [
        # Pro tier: Balanced models for 8-16GB VRAM
        "qwen3-vl:4b",              # VLM: Qwen3-VL-4B-Instruct
        "qwen3-embedding:0.6b",     # MV model: Qwen3-Embedding-0.6B
    ],
    "ultra": [
        # Ultra tier: High-end models for â‰¥20GB VRAM
        "qwen3-vl:8b",              # VLM: Qwen3-VL-8B-Instruct
        "qwen3-embedding:8b",       # MV model: Qwen3-Embedding-8B
    ]
}


def check_ollama() -> bool:
    """
    Ollama ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸.

    Returns:
        bool: Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ True
    """
    try:
        result = subprocess.run(
            ["ollama", "--version"],
            check=True,
            capture_output=True,
            text=True
        )
        version = result.stdout.strip()
        print(f"âœ… Ollama installed: {version}")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False


def check_ollama_server() -> bool:
    """
    Ollama ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸.

    Returns:
        bool: ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ True
    """
    try:
        result = subprocess.run(
            ["ollama", "list"],
            check=True,
            capture_output=True,
            text=True,
            timeout=5
        )
        return True
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        return False


def list_installed_models() -> List[str]:
    """
    í˜„ì¬ ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ.

    Returns:
        List[str]: ì„¤ì¹˜ëœ ëª¨ë¸ ì´ë¦„ ëª©ë¡
    """
    try:
        result = subprocess.run(
            ["ollama", "list"],
            check=True,
            capture_output=True,
            text=True
        )
        lines = result.stdout.strip().split('\n')[1:]  # Skip header
        models = []
        for line in lines:
            if line.strip():
                # Extract model name (first column)
                model_name = line.split()[0]
                models.append(model_name)
        return models
    except (subprocess.CalledProcessError, FileNotFoundError):
        return []


def pull_model(model: str, dry_run: bool = False) -> bool:
    """
    Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ.

    Args:
        model: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: qwen3-vl:4b)
        dry_run: Trueë©´ ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜

    Returns:
        bool: ì„±ê³µ ì‹œ True
    """
    print(f"\nğŸ“¥ Pulling {model}...")

    if dry_run:
        print(f"   [DRY RUN] Would execute: ollama pull {model}")
        return True

    try:
        # Use subprocess.run with real-time output
        process = subprocess.Popen(
            ["ollama", "pull", model],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        # Print output line by line
        for line in process.stdout:
            print(f"   {line.rstrip()}")

        process.wait()

        if process.returncode == 0:
            print(f"   âœ… {model} installed successfully")
            return True
        else:
            print(f"   âŒ {model} installation failed (exit code: {process.returncode})")
            return False

    except FileNotFoundError:
        print(f"   âŒ Ollama command not found")
        return False
    except Exception as e:
        print(f"   âŒ {model} installation error: {e}")
        return False


def setup_tier(tier: str, dry_run: bool = False, skip_installed: bool = True):
    """
    íŠ¹ì • í‹°ì–´ì˜ ëª¨ë¸ ì„¤ì¹˜.

    Args:
        tier: "standard" | "pro" | "ultra"
        dry_run: Trueë©´ ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜
        skip_installed: Trueë©´ ì´ë¯¸ ì„¤ì¹˜ëœ ëª¨ë¸ ê±´ë„ˆë›°ê¸°
    """
    models = TIER_MODELS.get(tier, [])

    if not models:
        print(f"âš ï¸  No Ollama models defined for tier '{tier}'")
        print(f"   Note: {tier.upper()} tier may use Transformers models (Hugging Face)")
        return

    print(f"\n{'='*60}")
    print(f"ğŸ“¦ Setting up {tier.upper()} tier ({len(models)} models)")
    print(f"{'='*60}")

    # Check already installed models
    installed = []
    if skip_installed:
        installed = list_installed_models()
        print(f"ğŸ“‹ Currently installed models: {len(installed)}")

    success_count = 0
    skip_count = 0
    fail_count = 0

    for model in models:
        # Check if already installed
        if skip_installed and model in installed:
            print(f"\nâ­ï¸  Skipping {model} (already installed)")
            skip_count += 1
            continue

        # Pull model
        if pull_model(model, dry_run):
            success_count += 1
        else:
            fail_count += 1

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Summary for {tier.upper()} tier:")
    print(f"   âœ… Installed: {success_count}")
    print(f"   â­ï¸  Skipped: {skip_count}")
    print(f"   âŒ Failed: {fail_count}")
    print(f"{'='*60}")


def main():
    parser = argparse.ArgumentParser(
        description="Setup Ollama models for ImageParser v3.1",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python tools/setup_models.py --tier=pro
  python tools/setup_models.py --all
  python tools/setup_models.py --all --dry-run
  python tools/setup_models.py --tier=ultra --force

Tiers:
  standard: Lightweight models (â‰¤6GB VRAM)
  pro:      Balanced models (8-16GB VRAM)
  ultra:    High-end models (â‰¥20GB VRAM)
        """
    )
    parser.add_argument(
        "--tier",
        choices=["standard", "pro", "ultra"],
        help="Setup specific tier"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Setup all tiers"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate installation without downloading"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force re-download even if models are installed"
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸš€ ImageParser v3.1 Model Setup")
    print("=" * 60)

    # Check Ollama installation
    if not check_ollama():
        print("\nâŒ Ollama not found!")
        print("\nğŸ’¡ Please install Ollama first:")
        print("   - Windows/Mac: https://ollama.com/download")
        print("   - Linux: curl https://ollama.com/install.sh | sh")
        sys.exit(1)

    # Check Ollama server
    if not args.dry_run and not check_ollama_server():
        print("\nâš ï¸  Ollama server is not running!")
        print("   Starting Ollama server in background...")
        print("   Please run: ollama serve")
        print("   Or just continue - ollama pull will start the server automatically")

    # Determine tiers to setup
    tiers_to_setup = []
    if args.all:
        tiers_to_setup = ["standard", "pro", "ultra"]
    elif args.tier:
        tiers_to_setup = [args.tier]
    else:
        print("\nâŒ Please specify --tier or --all")
        print("   Example: python tools/setup_models.py --tier=pro")
        sys.exit(1)

    # Setup each tier
    for tier in tiers_to_setup:
        setup_tier(tier, dry_run=args.dry_run, skip_installed=not args.force)

    print("\n" + "=" * 60)
    print("âœ… Model setup complete!")
    print("=" * 60)

    if not args.dry_run:
        print("\nğŸ’¡ Next steps:")
        print("   1. Verify installation: ollama list")
        print("   2. Start processing files:")
        print("      python backend/pipeline/ingest_engine.py --file \"path/to/image.psd\"")
        print("   3. Or auto-detect tier:")
        print("      python -c \"from backend.utils.tier_config import get_active_tier; print(get_active_tier())\"")
    print()


if __name__ == "__main__":
    main()
